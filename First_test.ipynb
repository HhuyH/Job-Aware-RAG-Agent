{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports libary\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare AI model gpt-4o-mini is the most lowest cost model\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99db665f",
   "metadata": {},
   "source": [
    "# First test with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0652c2-3d76-40c7-8313-9dc1895155a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test matching documents\n",
    "\n",
    "context = {}\n",
    "\n",
    "documents = glob.glob(\"knowledge_base/Deep_learning/*\")\n",
    "\n",
    "for document in documents:\n",
    "    name = document.split(os.sep)[-1][:-3]\n",
    "    doc = \"\"\n",
    "    with open(document, \"r\", encoding=\"utf-8\") as f:\n",
    "        doc = f.read()\n",
    "    context[name]=doc\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aba46a57-d973-4195-8fe3-70fc60687192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['association_rule_learning', 'clustering', 'decision_tree', 'dimensionality_reduction', 'gradient_boosting', 'knn', 'linear_regression', 'logistic_regression', 'machine_learning', 'machine_learning_overview', 'naive_bayes', 'random_forest', 'regression_classification', 'reinforcement_learning ', 'supervised_learning', 'svm', 'unsupervised_learning'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "129c7d1e-0094-4479-9459-f9360b95f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an expert in AI/ML career guidance. You must answer strictly based on the context provided from the database.\n",
    "If the context does not contain the necessary information, or if the answer cannot be derived directly from the context,\n",
    "you must respond: \"I do not have information in the database to answer this question.\"\n",
    "\n",
    "Rules:\n",
    "- Do not invent, guess, or hallucinate.\n",
    "- Do not rely on prior knowledge outside the provided context.\n",
    "- Provide brief, accurate answers only.\n",
    "- If the context is empty or irrelevant, say you don't have the data.\n",
    "- All answers must be in Vietnamese.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d40e390b-c110-42d5-8d80-daf3295b9862",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_context(message):\n",
    "    relevant_context = []\n",
    "    for context_title, context_details in context.items():\n",
    "        if context_title.lower() in message.lower():\n",
    "            relevant_context.append(context_details)\n",
    "    return relevant_context          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d126cfcb-e85c-4dd9-837e-9d2b8436d4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---\\ntitle: \"Decision Tree\"\\ndescription: \"Thuật toán Decision Tree trong Machine Learning.\"\\ntags: [\"Machine Learning\", \"Decision Tree\"]\\n---\\n\\n# Decision Tree\\n\\n## 1. Khái niệm\\n\\nDecision Tree là thuật toán thuộc nhóm **Supervised Learning**, dùng cho cả **Classification** và **Regression**. Mô hình hoạt động bằng cách chia dữ liệu thành các nhánh dựa trên điều kiện, tạo thành cấu trúc giống cây gồm **node**, **branch**, và **leaf**.\\n\\n---\\n\\n## 2. Cấu trúc cây quyết định\\n\\n* **Root Node**: điểm bắt đầu, chứa toàn bộ dữ liệu.\\n* **Internal Node**: nút kiểm tra điều kiện (feature).\\n* **Leaf Node**: nút kết luận → class label hoặc giá trị dự đoán.\\n* **Branch**: đường nối thể hiện lựa chọn theo điều kiện.\\n\\n---\\n\\n## 3. Cách hoạt động\\n\\n1. Chọn feature tốt nhất để chia dữ liệu.\\n2. Tạo một node với điều kiện rẽ nhánh.\\n3. Lặp lại cho từng nhánh đến khi đạt điều kiện dừng.\\n4. Node cuối trở thành leaf node.\\n\\n---\\n\\n## 4. Tiêu chí chọn feature (Split Criteria)\\n\\n### Đối với Classification\\n\\n* **Gini Impurity**\\n* **Entropy / Information Gain**\\n\\n### Đối với Regression\\n\\n* **MSE (Mean Squared Error)**\\n* **MAE (Mean Absolute Error)**\\n\\n---\\n\\n## 5. Ưu điểm\\n\\n* Dễ hiểu, trực quan.\\n* Không cần chuẩn hóa dữ liệu.\\n* Xử lý được dữ liệu dạng số và dạng phân loại.\\n* Hoạt động tốt khi quan hệ phi tuyến.\\n\\n---\\n\\n## 6. Nhược điểm\\n\\n* Dễ overfitting nếu không cắt tỉa (pruning).\\n* Nhạy với nhiễu và outlier.\\n* Thay đổi nhỏ trong dữ liệu có thể làm thay đổi cấu trúc cây.\\n\\n---\\n\\n## 7. Kỹ thuật cải thiện\\n\\n* **Pruning (Cắt tỉa)**: giảm độ sâu cây để tránh overfitting.\\n* **Max Depth**: giới hạn chiều cao cây.\\n* **Min Samples Split / Leaf**: yêu cầu số mẫu tối thiểu để split.\\n* **Feature Importance**: đánh giá độ quan trọng của từng feature.\\n\\n---\\n\\n## 8. Ứng dụng thực tế\\n\\n* Phân loại khách hàng.\\n* Chẩn đoán bệnh.\\n* Dự đoán rủi ro tín dụng.\\n* Hệ thống hỗ trợ ra quyết định.\\n\\n---\\n\\n## 9. Cách nhận biết khi nào dùng Decision Tree\\n\\nDùng khi:\\n\\n* Cần mô hình dễ diễn giải.\\n* Dữ liệu có quan hệ phi tuyến.\\n* Không muốn chuẩn hóa dữ liệu.\\n* Cần giải thích quyết định cho stakeholder.\\n\\n---\\n\\n## 10. Metric đánh giá\\n\\n### Đối với Classification\\n\\n* Accuracy\\n* Precision / Recall / F1\\n* Confusion Matrix\\n\\n### Đối với Regression\\n\\n* MSE / RMSE\\n* MAE\\n* R² Score\\n\\n---\\n\\n## 11. Cách trả lời phỏng vấn (chốt gọn)\\n\\n* Decision Tree là mô hình phân loại hoặc hồi quy dựa trên việc chia dữ liệu thành các nhánh theo điều kiện.\\n* Dễ hiểu, trực quan, nhưng dễ overfit nếu không cắt tỉa.\\n* Tiêu chí split: Gini / Entropy cho classification, MSE cho regression.\\n']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_relevant_context(\"what about decision_tree?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7cef7f-f214-4bac-8217-3f9ab9ba1bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context(message):\n",
    "    relevant_context = get_relevant_context(message)\n",
    "    if not relevant_context:\n",
    "        message += \"\\n\\n[CONTEXT NOT FOUND IN DATABASE]\"\n",
    "    \n",
    "    message += \"\\n\\nThe following additional context might be relevant in answering this question:\\n\\n\"\n",
    "    for relevant in relevant_context:\n",
    "            message += relevant + \"\\n\\n\"\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b36399c-440b-4049-9d39-68d208283c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what about decision tree?\n"
     ]
    }
   ],
   "source": [
    "print(add_context(\"what about decision tree?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "968e7bf2-e862-4679-a11f-6c1efb6ec8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_message}] + history\n",
    "    message = add_context(message)\n",
    "    messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    stream = openai.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    response = \"\"\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb659-13ce-47ab-8a5e-01b930494964",
   "metadata": {},
   "source": [
    "## Now we will bring this up in Gradio using the Chat interface -\n",
    "\n",
    "A quick and easy way to prototype a chat with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3536590-85c7-4155-bd87-ae78a1467670",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.ChatInterface(chat).launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeba7bb",
   "metadata": {},
   "source": [
    "# Visualizing the Vector Store conntect with langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728d4686",
   "metadata": {},
   "source": [
    "## Split the text into chunks and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7706b982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain and Chroma and plotly\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d04703c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(\"knowledge_base/*\")\n",
    "\n",
    "# With thanks to CG and Jon R, students on the course, for this fix needed for some users \n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd18a118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of document chunks: 437\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"Number of document chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53aac4f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: DSA, Python, OOP, Deep_Learning, Preprocessing, Machine_Learning\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3dd1c399",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    if 'hình ảnh, âm thanh' in chunk.page_content:\n",
    "        print(chunk)\n",
    "        print(\"_________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48873d11-2fbd-4329-af27-46c781788561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b3656e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: DSA, Python, OOP, Deep_Learning, Preprocessing, Machine_Learning\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27aff59",
   "metadata": {},
   "source": [
    "### Embeddings, and \"Auto-Encoding LLMs\"\n",
    "mapping each chunk of text into a Vector that represents the meaning of the text, known as an embedding.\n",
    "\n",
    "OpenAI offers a model to do this, which will use by calling their API with some LangChain code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d1ce24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 437 documents\n"
     ]
    }
   ],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# If you would rather use the free Vector Embeddings from HuggingFace sentence-transformers\n",
    "# Then replace embeddings = OpenAIEmbeddings()\n",
    "# with:\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Delete if already exists\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96d29e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 1,536 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9d6b2f",
   "metadata": {},
   "source": [
    "## Visualizing the Vector Store\n",
    "Only run on notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e1ded1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionGetEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    }
   ],
   "source": [
    "# Prework\n",
    "\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
    "color_map = {\n",
    "    'Preprocessing': 'blue',\n",
    "    'Deep_Learning': 'green',\n",
    "    'Machine_Learning': 'red',\n",
    "    'DSA': 'orange',\n",
    "    'Python': 'yellow',\n",
    "    'OOP': 'black'\n",
    "}\n",
    "\n",
    "colors = [color_map.get(t, 'gray') for t in doc_types]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea67e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We humans find it easier to visalize things in 2D!\n",
    "# Reduce the dimensionality of the vectors to 2D using t-SNE\n",
    "# (t-distributed stochastic neighbor embedding)\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='2D Chroma Vector Store Visualization',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='y',\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook_connected\"\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13adc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try 3D!\n",
    "\n",
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae597b79",
   "metadata": {},
   "source": [
    "## Use LangChain to bring it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c88eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfc29f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN, or Convolutional Neural Network, is a specialized type of neural network particularly effective for processing data with a grid-like topology, such as images. It utilizes convolutional layers to automatically learn spatial features from the input data, making it suitable for tasks like image classification, object detection, and video analysis. CNNs are designed to reduce the number of parameters compared to traditional fully connected networks (MLPs) by employing techniques like parameter sharing and local receptive fields.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you describe cnn in a few sentences\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "90f4ca7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e401a55",
   "metadata": {},
   "source": [
    "### Now we will bring this up in Gradio using the Chat interface -\n",
    "\n",
    "A quick and easy way to prototype a chat with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "863e297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping in a function - note that history isn't used, as the memory is in the conversation_chain\n",
    "\n",
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c073d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And in Gradio:\n",
    "\n",
    "view = gr.ChatInterface(chat).launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1d0cb61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationalRetrievalChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question. \n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "---\n",
      "title: Deep Learning là gì?\n",
      "description: Ghi chú tổng quan về Deep Learning, khái niệm cốt lõi, cách hoạt động, cấu trúc mô hình, ví dụ code, lỗi thường gặp và ứng dụng thực tế.\n",
      "tags: [Deep Learning, Neural Networks, Machine Learning, AI]\n",
      "---\n",
      "\n",
      "## 1. Khái niệm\n",
      "\n",
      "**Deep Learning (DL)** là một nhánh của **Machine Learning**, sử dụng **mạng nơ-ron nhân tạo nhiều tầng (deep neural networks)** để học các biểu diễn phức tạp từ dữ liệu.\n",
      "\n",
      "Điểm khác biệt cốt lõi:\n",
      "- Machine Learning truyền thống: **feature engineering thủ công**\n",
      "- Deep Learning: **tự động học feature từ dữ liệu thô**\n",
      "\n",
      "Deep Learning đặc biệt hiệu quả với:\n",
      "- Dữ liệu lớn\n",
      "- Dữ liệu phi cấu trúc: ảnh, âm thanh, văn bản, video\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Mục đích & Khi nào dùng (Use Cases)\n",
      "\n",
      "### Mục đích\n",
      "- Mô hình hóa các quan hệ **phi tuyến phức tạp**\n",
      "- Trích xuất đặc trưng ở nhiều mức trừu tượng\n",
      "\n",
      "Trong thực tế, **DL hiếm khi đứng một mình**, mà là một phần của hệ thống lớn hơn.\n",
      "\n",
      "---\n",
      "\n",
      "## 9. Câu hỏi phỏng vấn (Interview Questions)\n",
      "\n",
      "1. Deep Learning khác gì Machine Learning?\n",
      "2. Tại sao cần hàm activation?\n",
      "3. Vai trò của backpropagation?\n",
      "4. Khi nào nên dùng DL thay vì ML truyền thống?\n",
      "5. Vì sao DL cần nhiều dữ liệu?\n",
      "6. Overfitting trong DL xảy ra khi nào?\n",
      "\n",
      "---\n",
      "\n",
      "## 10. TL;DR\n",
      "\n",
      "* Deep Learning = Neural Network nhiều tầng\n",
      "* Tự học feature, xử lý tốt dữ liệu phức tạp\n",
      "* Mạnh nhưng tốn tài nguyên\n",
      "* Không phải bài toán nào cũng cần DL\n",
      "* Hiểu bản chất > chạy model\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Mục đích & Khi nào dùng (Use Cases)\n",
      "\n",
      "### Mục đích\n",
      "- Mô hình hóa các quan hệ **phi tuyến phức tạp**\n",
      "- Trích xuất đặc trưng ở nhiều mức trừu tượng\n",
      "\n",
      "### Khi nên dùng Deep Learning\n",
      "- Dataset đủ lớn (hàng chục nghìn mẫu trở lên)\n",
      "- Bài toán có cấu trúc phức tạp:\n",
      "  - Image / Speech / NLP\n",
      "- Feature khó thiết kế bằng tay\n",
      "\n",
      "### Khi KHÔNG nên dùng\n",
      "- Dataset nhỏ\n",
      "- Bài toán tuyến tính, đơn giản\n",
      "- Yêu cầu giải thích mô hình cao (interpretability)\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Cách hoạt động bên trong (Internal Logic)\n",
      "\n",
      "Một mô hình Deep Learning gồm nhiều tầng:\n",
      "\n",
      "- **Input layer**\n",
      "- **Hidden layers**\n",
      "- **Output layer**\n",
      "\n",
      "Mỗi neuron thực hiện:\n",
      "\n",
      "$$\n",
      "z = w^T x + b\n",
      "$$\n",
      "\n",
      "$$\n",
      "a = f(z)\n",
      "$$\n",
      "\n",
      "Trong đó:\n",
      "- \\( w \\): trọng số\n",
      "- \\( b \\): bias\n",
      "- \\( f \\): hàm kích hoạt (ReLU, Sigmoid, Softmax...)\n",
      "\n",
      "### Quá trình huấn luyện\n",
      "1. Forward propagation\n",
      "2. Tính loss\n",
      "3. Backpropagation\n",
      "4. Cập nhật trọng số (Gradient Descent)\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Cấu trúc / Cú pháp (Structure)\n",
      "\n",
      "Một pipeline Deep Learning tiêu chuẩn:\n",
      "\n",
      "* training loss thấp\n",
      "  * generalization tốt\n",
      "\n",
      "---\n",
      "\n",
      "## 7. So sánh với khái niệm liên quan (Comparison)\n",
      "\n",
      "| Tiêu chí            | Machine Learning | Deep Learning |\n",
      "| ------------------- | ---------------- | ------------- |\n",
      "| Feature engineering | Thủ công         | Tự động       |\n",
      "| Dữ liệu             | Nhỏ–vừa          | Lớn           |\n",
      "| Độ phức tạp         | Thấp–trung bình  | Cao           |\n",
      "| Tài nguyên          | Ít               | GPU / TPU     |\n",
      "\n",
      "---\n",
      "\n",
      "## 8. Ứng dụng trong thực tế (Practical Insights)\n",
      "\n",
      "* Computer Vision: CNN\n",
      "* NLP: RNN, LSTM, Transformer\n",
      "* Speech Recognition\n",
      "* Recommendation Systems\n",
      "* Autonomous Driving\n",
      "* AI Agent / RAG (embedding, encoder models)\n",
      "\n",
      "Trong thực tế, **DL hiếm khi đứng một mình**, mà là một phần của hệ thống lớn hơn.\n",
      "\n",
      "---\n",
      "\n",
      "## 9. Câu hỏi phỏng vấn (Interview Questions)\n",
      "Human: Khi nào nên sử dụng deep learning?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "Answer: Bạn nên sử dụng Deep Learning khi:\n",
      "\n",
      "- Dataset đủ lớn (hàng chục nghìn mẫu trở lên)\n",
      "- Bài toán có cấu trúc phức tạp, như:\n",
      "  - Xử lý hình ảnh (Image)\n",
      "  - Nhận diện giọng nói (Speech)\n",
      "  - Xử lý ngôn ngữ tự nhiên (NLP)\n",
      "- Các đặc trưng (features) khó thiết kế bằng tay. \n",
      "\n",
      "Ngược lại, không nên sử dụng Deep Learning khi dataset nhỏ, bài toán tuyến tính hay đơn giản, hoặc khi yêu cầu giải thích mô hình cao (interpretability).\n"
     ]
    }
   ],
   "source": [
    "# Let's investigate what gets sent behind the scenes\n",
    "\n",
    "from langchain_core.callbacks import StdOutCallbackHandler\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory, callbacks=[StdOutCallbackHandler()])\n",
    "\n",
    "query = \"Khi nào nên sử dụng deep learning?\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "909e42e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG; k is how many chunks to use\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c8107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34cce637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat).launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d33ce2",
   "metadata": {},
   "source": [
    "## Fix replay with pretrained knowledge\n",
    "\n",
    "Base prompt of langchain\n",
    "\n",
    "“Use the following context to answer the question.\n",
    "If you don’t know, say you don’t know.”\n",
    "\n",
    "but it don't have this\n",
    "“Do not answer if the question is general knowledge.”\n",
    "\n",
    "Có 1 số kiến thức không được thể hiện trên knowledge tự build nhưng có đề cập thì nó vẫn láy ra những kiến thức có sản được train trong model vì vậy cần sửa prompt để tránh tính trạng này\n",
    "\n",
    "## Cách này phù hộp cho việc sử dung cho\n",
    "- RAG nội bộ\n",
    "- QA system cần độ tin cậy cao\n",
    "\n",
    "*CÁCH NÀY SẼ KO ĐƯỢC SỬ DỤNG CHO DỰ ÁN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c556ba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import LLMChain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6142ba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template for question answering that strictly limits the LLM to only use provided context\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a question-answering system that MUST rely ONLY on the provided data.\n",
    "\n",
    "MANDATORY RULES:\n",
    "- Use ONLY the information contained in the <context> section.\n",
    "- DO NOT use any prior knowledge, pretrained knowledge, or external information.\n",
    "- Do NOT infer, extrapolate, or add explanations.\n",
    "- If the answer does NOT appear explicitly or cannot be clearly derived from <context>,\n",
    "  respond with EXACTLY the following sentence and NOTHING ELSE:\n",
    "\n",
    "\"I do not have information in the database to answer this question.\"\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85fd150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a question generator chain to rephrase the user's question based on chat history\n",
    "qa_chain = load_qa_chain(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    prompt=QA_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab30cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template to rephrase the user's question based on chat history\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"question\"],\n",
    "    template=\"\"\"\n",
    "Given the following conversation history and a follow-up question,\n",
    "rewrite the follow-up question into a standalone question.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "Follow-up Question:\n",
    "{question}\n",
    "\n",
    "Standalone Question:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995893cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\Job-Aware-RAG-Agent\\.venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Create the question generator LLM chain\n",
    "question_generator = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=CONDENSE_QUESTION_PROMPT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f84ef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG; k is how many chunks to use\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 3.5 LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain(\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain=qa_chain,\n",
    "    question_generator=question_generator,\n",
    "    memory=memory\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b9772ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping in a function - note that history isn't used, as the memory is in the conversation_chain\n",
    "def chat(question, history):\n",
    "    result = conversation_chain.invoke({\"question\": question})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8839f88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat).launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c6acb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
