{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "## Expert Knowledge Worker\n",
    "\n",
    "This project will use RAG (Retrieval Augmented Generation) to ensure our question/answering assistant has high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Code\\Job-Aware-RAG-Agent\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports libary\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "730711a9-6ffe-4eee-8f48-d6cfb7314905",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob(\"knowledge_base/*\")\n",
    "\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "252f17e9-3529-4e81-996c-cfa9f08e75a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e8decb0-d9b0-4d51-8402-7a6174d22159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'knowledge_base\\\\Deep_Learning\\\\fundamentals\\\\weight_initialization.md', 'doc_type': 'Deep_Learning'}, page_content='---\\ntitle: Weight Initialization\\ndescription: Giải thích các phương pháp khởi tạo trọng số trong neural network, công thức toán, tác động tới huấn luyện và cách áp dụng.\\ntags: [deep-learning, fundamentals, neural-network, weight-initialization]\\n---\\n\\n# Weight Initialization\\n\\n## 1. Tóm tắt khái niệm (Definition)\\nWeight initialization là quá trình **gán giá trị ban đầu cho trọng số của neural network** trước khi huấn luyện.  \\nMục tiêu: **tránh gradient vanish/explode, giúp hội tụ nhanh hơn**.\\n\\n---\\n\\n## 2. Mục đích & khi nào dùng (Use Cases)\\n- Tránh **vanishing/exploding gradient**.  \\n- Tăng tốc hội tụ khi huấn luyện.  \\n- Giúp mạng học biểu diễn phi tuyến tốt hơn.  \\n- Dùng trước bước training, cho mọi loại layer dense, convolutional, RNN.\\n\\n---\\n\\n## 3. Các phương pháp phổ biến\\n\\n### 3.1 Zero Initialization\\n```\\n\\nW = 0\\n\\n```\\n- Không nên dùng cho hidden layers → mọi neuron học giống nhau.  \\n- Có thể dùng cho bias.\\n\\n---\\n\\n### 3.2 Random Initialization\\n\\n$$\\nW \\\\sim U(-a, a) \\\\text{ hoặc } N(0, \\\\sigma^2)\\n$$\\n\\n- Random nhỏ giúp symmetry break.  \\n- Chọn a hoặc σ quá lớn → gradient explode.\\n\\n---\\n\\n### 3.3 Xavier / Glorot Initialization\\n- Cho activation **tanh hoặc sigmoid**:\\n\\n$$\\nW \\\\sim U\\\\left(-\\\\sqrt{\\\\frac{6}{n_{in} + n_{out}}}, \\\\sqrt{\\\\frac{6}{n_{in} + n_{out}}}\\\\right)\\n$$\\n\\nhoặc Gaussian:\\n\\n$$\\nW \\\\sim N\\\\left(0, \\\\frac{2}{n_{in} + n_{out}}\\\\right)\\n$$\\n\\n- \\\\( n_{in} \\\\): số neuron input  \\n- \\\\( n_{out} \\\\): số neuron output\\n\\n---\\n\\n### 3.4 He Initialization\\n- Cho activation **ReLU / Leaky ReLU**:\\n\\n$$\\nW \\\\sim N\\\\left(0, \\\\frac{2}{n_{in}}\\\\right)\\n$$\\n\\nhoặc Uniform:\\n\\n$$\\nW \\\\sim U\\\\left(-\\\\sqrt{\\\\frac{6}{n_{in}}}, \\\\sqrt{\\\\frac{6}{n_{in}}}\\\\right)\\n$$\\n\\n- Giúp gradient không vanish/explode khi ReLU.\\n\\n---\\n\\n## 4. Cấu trúc / Cú pháp (Syntax / Structure)\\n\\n### PyTorch Example\\n\\n```python\\nimport torch\\nimport torch.nn as nn\\n\\nlayer = nn.Linear(3, 5)\\n\\n# Xavier\\nnn.init.xavier_uniform_(layer.weight)\\n\\n# He\\nnn.init.kaiming_normal_(layer.weight, nonlinearity=\\'relu\\')\\n\\n# Bias = 0\\nnn.init.zeros_(layer.bias)\\n```\\n\\n---\\n\\n## 5. Ví dụ code (Code Examples)\\n\\n```python\\nimport torch\\n\\n# Dense layer\\nW = torch.empty(4,3)\\n\\n# Xavier uniform\\nnn.init.xavier_uniform_(W)\\nprint(\"Xavier init:\", W)\\n\\n# He normal\\nnn.init.kaiming_normal_(W, nonlinearity=\\'relu\\')\\nprint(\"He init:\", W)\\n```\\n\\n---\\n\\n## 6. Lỗi thường gặp (Common Pitfalls)\\n\\n* Zero init hidden layer → symmetry → mọi neuron học giống nhau.\\n* Random init quá lớn → gradient explode.\\n* Chọn init không phù hợp với activation → vanishing/exploding gradient.\\n* Quên init bias → ảnh hưởng convergence.\\n\\n---\\n\\n## 7. So sánh với khái niệm liên quan (Comparison)\\n\\n| Method | Activation phù hợp | Ưu điểm                            | Nhược điểm                  |\\n| ------ | ------------------ | ---------------------------------- | --------------------------- |\\n| Zero   | Bias               | Đơn giản                           | Hidden neuron giống nhau    |\\n| Random | Tất cả             | Break symmetry                     | Cần scale nhỏ               |\\n| Xavier | tanh, sigmoid      | Gradient ổn định                   | Không tốt với ReLU          |\\n| He     | ReLU, Leaky ReLU   | Tránh vanishing/exploding gradient | Không dùng cho tanh/sigmoid |\\n\\n---\\n\\n## 8. Ứng dụng trong thực tế (Practical Insights)\\n\\n* Hidden layers → Xavier (tanh/sigmoid) hoặc He (ReLU).\\n* Bias thường khởi tạo = 0.\\n* Conv layers → dùng He cho ReLU.\\n* RNN / LSTM → Xavier cho input-hidden, He cho hidden-hidden nếu ReLU.\\n\\n---\\n\\n## 9. Câu hỏi phỏng vấn (Interview Questions)\\n\\n* Vì sao không khởi tạo zero cho hidden layers?\\n* Khi nào dùng Xavier, khi nào dùng He?\\n* Random init quá lớn ảnh hưởng gì tới gradient?\\n* Bias có nên khởi tạo khác 0 không?\\n* Khởi tạo weight tốt giúp training nhanh hơn như thế nào?\\n\\n---\\n\\n## 10. TL;DR (Short Summary)\\n\\n* Weight initialization quyết định gradient ban đầu.\\n* Xavier → sigmoid/tanh, He → ReLU.\\n* Zero init hidden layer → tránh.\\n* Chọn init phù hợp activation giúp mạng hội tụ nhanh, tránh vanishing/exploding gradient.\\n\\n\\n')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7310c9c8-03c1-4efc-a104-5e89aec6db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd06e02f-6d9b-44cc-a43d-e1faa8acc7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d2562754-9052-4aae-92c1-37236435ea06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'knowledge_base\\\\Deep_Learning\\\\architectures\\\\cnn_architectures.md', 'doc_type': 'Deep_Learning'}, page_content='---\\n\\n## 7. So sánh với khái niệm liên quan (Comparison)\\n\\n* Basic CNN vs ResNet vs VGG vs Inception:\\n\\n  * Basic CNN: layer nối tiếp, đơn giản.\\n  * VGG: nhiều layer nhỏ 3x3, đơn giản nhưng sâu.\\n  * ResNet: skip connections, giảm vanishing gradient.\\n  * Inception: multi-scale filter, giảm tham số mà giữ hiệu suất.\\n\\n---\\n\\n## 8. Ứng dụng trong thực tế (Practical Insights)\\n\\n* VGG/ResNet/Inception dùng trong ImageNet competition.\\n* ResNet thường dùng cho các dự án detection/segmentation.\\n* Inception hoặc EfficientNet tối ưu tốc độ và accuracy.\\n\\n---\\n\\n## 9. Câu hỏi phỏng vấn (Interview Questions)\\n\\n* Các kiến trúc CNN khác nhau điểm gì?\\n* Skip connection giúp gì trong ResNet?\\n* BatchNorm hoạt động thế nào?\\n* Khi nào chọn VGG thay vì ResNet?\\n\\n---\\n\\n## 10. TL;DR (Short Summary)')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c54b4b6-06da-463d-bee7-4dd456c2b887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document types found: Deep_Learning, OOP, Preprocessing, DSA, Machine_Learning, Python\n"
     ]
    }
   ],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "128c73f7-f149-4904-a554-8140941fce0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='---\n",
      "\n",
      "## 7. So sánh với khái niệm liên quan (Comparison)\n",
      "\n",
      "| Optimizer      | Khi nào dùng          | Ưu điểm                | Nhược điểm                    |\n",
      "| -------------- | --------------------- | ---------------------- | ----------------------------- |\n",
      "| SGD            | Dữ liệu nhỏ, đơn giản | Dễ hiểu                | Chậm, oscillation             |\n",
      "| SGD + Momentum | Thường xuyên          | Hội tụ nhanh hơn       | Thêm hyperparameter           |\n",
      "| NAG            | Thường xuyên          | Dự đoán gradient trước | Thêm tính toán                |\n",
      "| AdaGrad        | Sparse features       | LR tự điều chỉnh       | LR giảm quá nhanh             |\n",
      "| RMSProp        | RNN / LSTM            | Giữ LR ổn định         | Hyperparameter cần tinh chỉnh |\n",
      "| Adam           | Hầu hết NN hiện đại   | Nhanh, ổn định         | Hyperparameter cần tune       |\n",
      "\n",
      "---\n",
      "\n",
      "## 8. Ứng dụng trong thực tế (Practical Insights)' metadata={'source': 'knowledge_base\\\\Deep_Learning\\\\fundamentals\\\\optimization_algorithms.md', 'doc_type': 'Deep_Learning'}\n",
      "_________\n",
      "page_content='**Ưu điểm**:\n",
      "\n",
      "* Linh hoạt hơn K-Means.\n",
      "* Mô hình hóa cụm không hình cầu.\n",
      "\n",
      "**Nhược điểm**:\n",
      "\n",
      "* Nhạy với khởi tạo.\n",
      "* Tốn chi phí tối ưu.\n",
      "\n",
      "---\n",
      "\n",
      "### 3.5. K-Means++\n",
      "\n",
      "**Ý tưởng**: Cải thiện bước khởi tạo centroid của K-Means nhằm giảm nguy cơ rơi vào nghiệm kém.\n",
      "\n",
      "**Cách hoạt động**:\n",
      "\n",
      "1. Chọn ngẫu nhiên 1 centroid đầu tiên.\n",
      "2. Với mỗi điểm còn lại, tính xác suất chọn làm centroid mới tỉ lệ thuận với bình phương khoảng cách đến centroid gần nhất.\n",
      "3. Lặp cho đến khi có đủ (k) centroid.\n",
      "4. Chạy K-Means như bình thường.\n",
      "\n",
      "**Ưu điểm**:\n",
      "\n",
      "* Hội tụ nhanh hơn.\n",
      "* Ít rủi ro rơi vào local minima.\n",
      "* Chất lượng cụm tốt hơn K-Means thường.\n",
      "\n",
      "**Nhược điểm**:\n",
      "\n",
      "* Tốn thời gian khởi tạo hơn.\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Bảng so sánh nhanh' metadata={'source': 'knowledge_base\\\\Machine_Learning\\\\clustering.md', 'doc_type': 'Machine_Learning'}\n",
      "_________\n"
     ]
    }
   ],
   "source": [
    "for chunk in chunks:\n",
    "    if 'Hội tụ' in chunk.page_content:\n",
    "        print(chunk)\n",
    "        print(\"_________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eca55f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
