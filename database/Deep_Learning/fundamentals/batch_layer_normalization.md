---
title: Batch Normalization & Layer Normalization
description: Gi·∫£i th√≠ch hai k·ªπ thu·∫≠t chu·∫©n h√≥a ph·ªï bi·∫øn trong Deep Learning, c∆° ch·∫ø ho·∫°t ƒë·ªông, c√¥ng th·ª©c, ·ª©ng d·ª•ng, l·ªói th∆∞·ªùng g·∫∑p v√† so s√°nh.
tags: [deep-learning, normalization, training-techniques, fundamentals]
---

## üìå M√¥ t·∫£ c√°ch m√¨nh ƒëang x√¢y d·ª±ng h·ªá th·ªëng file `.md`
(ƒê·ªÉ tr·ªëng ho·∫∑c gi·ªØ nguy√™n trong repo ‚Äî ph·∫ßn n√†y b·∫°n ƒë√£ chu·∫©n h√≥a r·ªìi)

---

# Batch Normalization & Layer Normalization

## 1. T√≥m t·∫Øt kh√°i ni·ªám (Definition)

**Batch Normalization (BN):** Chu·∫©n h√≥a gi√° tr·ªã k√≠ch ho·∫°t theo t·ª´ng batch trong qu√° tr√¨nh training, gi√∫p m√¥ h√¨nh h·ªôi t·ª• nhanh h∆°n v√† ·ªïn ƒë·ªãnh h∆°n.  
**Layer Normalization (LN):** Chu·∫©n h√≥a theo t·ª´ng sample tr√™n to√†n b·ªô chi·ªÅu feature, kh√¥ng ph·ª• thu·ªôc batch size.

---

## 2. M·ª•c ƒë√≠ch & khi n√†o d√πng (Use Cases)

- Gi·∫£m hi·ªán t∆∞·ª£ng internal covariate shift.  
- TƒÉng t·ªëc ƒë·ªô h·ªôi t·ª• khi training deep networks.  
- Gi√∫p gradient ·ªïn ƒë·ªãnh h∆°n.  
- V·ªõi m√¥ h√¨nh c√≥ batch nh·ªè (transformer, RNN): ∆∞u ti√™n **LayerNorm**.  
- V·ªõi CNN, batch l·ªõn: ∆∞u ti√™n **BatchNorm**.

---

## 3. C√°ch ho·∫°t ƒë·ªông b√™n trong (Internal Logic)

### ‚úî Batch Normalization  
Chu·∫©n h√≥a t·ª´ng feature d·ª±a tr√™n th·ªëng k√™ c·ªßa *c·∫£ batch*.

C√¥ng th·ª©c chu·∫©n h√≥a:

$$
\mu_B = \frac{1}{m} \sum_{i=1}^{m} x_i
$$

$$
\sigma_B^2 = \frac{1}{m} \sum_{i=1}^{m} (x_i - \mu_B)^2
$$

$$
\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}
$$

Th√™m scale & shift h·ªçc ƒë∆∞·ª£c:

$$
y_i = \gamma \hat{x}_i + \beta
$$

---

### ‚úî Layer Normalization  
Chu·∫©n h√≥a theo vector feature c·ªßa t·ª´ng sample:

$$
\mu = \frac{1}{H} \sum_{j=1}^{H} x_j
$$

$$
\sigma^2 = \frac{1}{H} \sum_{j=1}^{H} (x_j - \mu)^2
$$

$$
\hat{x}_j = \frac{x_j - \mu}{\sqrt{\sigma^2 + \epsilon}}
$$

$$
y_j = \gamma \hat{x}_j + \beta
$$

LN lu√¥n nh·∫•t qu√°n v√¨ kh√¥ng ph·ª• thu·ªôc batch size.

---

## 4. C·∫•u tr√∫c / C√∫ ph√°p (Syntax / Structure)

### PyTorch ‚Äì BatchNorm
```python
torch.nn.BatchNorm1d(num_features)
torch.nn.BatchNorm2d(num_features)
torch.nn.BatchNorm3d(num_features)
````

### PyTorch ‚Äì LayerNorm

```python
torch.nn.LayerNorm(normalized_shape)
```

---

## 5. V√≠ d·ª• code (Code Examples)

### BatchNorm trong CNN

```python
import torch.nn as nn

model = nn.Sequential(
    nn.Conv2d(32, 64, kernel_size=3, padding=1),
    nn.BatchNorm2d(64),
    nn.ReLU()
)
```

### LayerNorm trong Transformer

```python
import torch.nn as nn

model = nn.TransformerEncoderLayer(
    d_model=512,
    nhead=8,
    norm_first=True,  # s·ª≠ d·ª•ng LayerNorm tr∆∞·ªõc attention
)
```

---

## 6. L·ªói th∆∞·ªùng g·∫∑p (Common Pitfalls)

* Batch size qu√° nh·ªè ‚Üí BatchNorm ho·∫°t ƒë·ªông k√©m ·ªïn ƒë·ªãnh.
* Qu√™n chuy·ªÉn model sang `eval()` khi inference ‚Üí BN d√πng sai running mean/var.
* D√πng BN cho d·ªØ li·ªáu tu·∫ßn t·ª± (RNN) ‚Üí kh√¥ng ph√π h·ª£p.
* LayerNorm ch·∫≠m h∆°n BN tr√™n CNN v√¨ t√≠nh to√°n theo t·ª´ng sample.

---

## 7. So s√°nh v·ªõi kh√°i ni·ªám li√™n quan (Comparison)

### BatchNorm vs LayerNorm

| Ti√™u ch√≠              | BatchNorm          | LayerNorm               |
| --------------------- | ------------------ | ----------------------- |
| D·ª±a v√†o batch         | C√≥                 | Kh√¥ng                   |
| Ph·ªï bi·∫øn trong        | CNN                | Transformer, RNN        |
| Ph·ª• thu·ªôc batch size  | C√≥                 | Kh√¥ng                   |
| ·ªîn ƒë·ªãnh khi batch nh·ªè | K√©m                | T·ªët                     |
| Inference             | D√πng running stats | Kh√¥ng c·∫ßn running stats |

---

## 8. ·ª®ng d·ª•ng trong th·ª±c t·∫ø (Practical Insights)

* BN ph√π h·ª£p v·ªõi training **CNN tr√™n GPU** v·ªõi batch l·ªõn.
* LN l√† ti√™u chu·∫©n trong **Transformer**, **LLM**, **GPT**, **BERT**.
* LN gi√∫p m√¥ h√¨nh ·ªïn ƒë·ªãnh khi batch size nh·ªè ho·∫∑c bi·∫øn ƒë·ªông.
* K·∫øt h·ª£p normalization ƒë√∫ng c√°ch ‚Üí gi·∫£m nhu c·∫ßu learning rate nh·ªè.

---

## 9. C√¢u h·ªèi ph·ªèng v·∫•n (Interview Questions)

* V√¨ sao batch size nh·ªè l√†m BatchNorm k√©m hi·ªáu qu·∫£?
* Gi·∫£i th√≠ch s·ª± kh√°c nhau trong c√°ch t√≠nh mean/variance gi·ªØa BN v√† LN?
* T·∫°i sao Transformer kh√¥ng d√πng BatchNorm?
* BN c√≥ ho·∫°t ƒë·ªông kh√°c nhau gi·ªØa training v√† inference nh∆∞ th·∫ø n√†o?
* V√¨ sao LayerNorm kh√¥ng c·∫ßn running mean/var?

---

## 10. TL;DR (Short Summary)

* BN chu·∫©n h√≥a theo batch; LN chu·∫©n h√≥a theo features c·ªßa t·ª´ng sample.
* BN nhanh v√† hi·ªáu qu·∫£ cho CNN; LN ·ªïn ƒë·ªãnh v√† ph√π h·ª£p cho Transformer.
* LN kh√¥ng ph·ª• thu·ªôc batch size ‚Äî quan tr·ªçng v·ªõi m√¥ h√¨nh tu·∫ßn t·ª±.
* C·∫£ BN v√† LN ƒë·ªÅu c√≥ tham s·ªë h·ªçc ƒë∆∞·ª£c: Œ≥ v√† Œ≤.

